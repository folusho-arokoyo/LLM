{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3607ca01",
   "metadata": {
    "papermill": {
     "duration": 0.031169,
     "end_time": "2025-02-14T08:26:22.049594",
     "exception": false,
     "start_time": "2025-02-14T08:26:22.018425",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Lesson 6: Model Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d18dc7",
   "metadata": {
    "papermill": {
     "duration": 0.027241,
     "end_time": "2025-02-14T08:26:22.104237",
     "exception": false,
     "start_time": "2025-02-14T08:26:22.076996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this lesson, you will reinforce your understanding of the transformer architecture by exploring the decoder-only [model](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) `microsoft/Phi-3-mini-4k-instruct`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6e0ad5",
   "metadata": {
    "papermill": {
     "duration": 0.026519,
     "end_time": "2025-02-14T08:26:22.157419",
     "exception": false,
     "start_time": "2025-02-14T08:26:22.130900",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup\n",
    "\n",
    "We start with setting up the lab by installing the required libraries (`transformers` and `accelerate`) and ignoring the warnings. The `accelerate` library is required by the `Phi-3` model. But you don't need to worry about installing these libraries, the requirements for this lab are already installed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa74409d",
   "metadata": {
    "papermill": {
     "duration": 0.026329,
     "end_time": "2025-02-14T08:26:22.212424",
     "exception": false,
     "start_time": "2025-02-14T08:26:22.186095",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style=\"background-color:#fff6ff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> 💻 &nbsp; <b>Access <code>requirements.txt</code> file:</b> If you'd like to access the requirements file: 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>. For more help, please see the <em>\"Appendix – Tips, Help, and Download\"</em> Lesson.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c4bea73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T08:26:22.267559Z",
     "iopub.status.busy": "2025-02-14T08:26:22.267114Z",
     "iopub.status.idle": "2025-02-14T08:26:22.271999Z",
     "shell.execute_reply": "2025-02-14T08:26:22.271234Z"
    },
    "height": 45,
    "papermill": {
     "duration": 0.034217,
     "end_time": "2025-02-14T08:26:22.273370",
     "exception": false,
     "start_time": "2025-02-14T08:26:22.239153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install transformers>=4.46.1 accelerate>=0.31.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abdbe668",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T08:26:22.329192Z",
     "iopub.status.busy": "2025-02-14T08:26:22.328834Z",
     "iopub.status.idle": "2025-02-14T08:26:27.573231Z",
     "shell.execute_reply": "2025-02-14T08:26:27.572048Z"
    },
    "papermill": {
     "duration": 5.274487,
     "end_time": "2025-02-14T08:26:27.575163",
     "exception": false,
     "start_time": "2025-02-14T08:26:22.300676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers>=4.48.3 accelerate>=1.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9015f55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T08:26:27.644764Z",
     "iopub.status.busy": "2025-02-14T08:26:27.644183Z",
     "iopub.status.idle": "2025-02-14T08:26:27.650653Z",
     "shell.execute_reply": "2025-02-14T08:26:27.649232Z"
    },
    "height": 64,
    "papermill": {
     "duration": 0.048737,
     "end_time": "2025-02-14T08:26:27.652703",
     "exception": false,
     "start_time": "2025-02-14T08:26:27.603966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6292864f",
   "metadata": {
    "id": "W_23Z_do-faF",
    "papermill": {
     "duration": 0.027705,
     "end_time": "2025-02-14T08:26:27.716599",
     "exception": false,
     "start_time": "2025-02-14T08:26:27.688894",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f17322a",
   "metadata": {
    "papermill": {
     "duration": 0.027346,
     "end_time": "2025-02-14T08:26:27.771553",
     "exception": false,
     "start_time": "2025-02-14T08:26:27.744207",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's first load the model and its tokenizer. For that you will first import the classes: `AutoModelForCausalLM` and `AutoTokenizer`. When you want to process a sentence, you can apply the tokenizer first and then the model in two separate steps. Or you can create a pipeline object that wraps the two steps and then apply the pipeline to the sentence. You'll explore both approaches in this notebook. This is why you'll also import the `pipeline` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c062fc3d",
   "metadata": {
    "papermill": {
     "duration": 0.027911,
     "end_time": "2025-02-14T08:26:27.826952",
     "exception": false,
     "start_time": "2025-02-14T08:26:27.799041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style=\"background-color:#fff1d7; padding:15px; \"> <b>FYI: </b> The transformers library has two types of model classes: <code> AutoModelForCausalLM </code> and <code>AutoModelForMaskedLM</code>. Causal language models represent the decoder-only models that are used for text generation. They are described as causal, because to predict the next token, the model can only attend to the preceding left tokens. Masked language models represent the encoder-only models that are used for rich text representation. They are described as masked, because they are trained to predict a masked or hidden token in a sequence.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f79da43f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T08:26:27.883863Z",
     "iopub.status.busy": "2025-02-14T08:26:27.883496Z",
     "iopub.status.idle": "2025-02-14T08:26:54.519847Z",
     "shell.execute_reply": "2025-02-14T08:26:54.518784Z"
    },
    "height": 62,
    "papermill": {
     "duration": 26.66778,
     "end_time": "2025-02-14T08:26:54.521787",
     "exception": false,
     "start_time": "2025-02-14T08:26:27.854007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import the required classes\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed33e9c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T08:26:54.576120Z",
     "iopub.status.busy": "2025-02-14T08:26:54.575520Z",
     "iopub.status.idle": "2025-02-14T08:26:56.516305Z",
     "shell.execute_reply": "2025-02-14T08:26:56.515428Z"
    },
    "executionInfo": {
     "elapsed": 130259,
     "status": "ok",
     "timestamp": 1718959891215,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "height": 181,
    "id": "-5RLd6dI-Ytm",
    "outputId": "fb085ff7-e06f-4142-8e95-5ff98b212e37",
    "papermill": {
     "duration": 1.969641,
     "end_time": "2025-02-14T08:26:56.517956",
     "exception": false,
     "start_time": "2025-02-14T08:26:54.548315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd9efba86cc491e80f24f7ab6a41bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/phi-3/pytorch/phi-3.5-mini-instruct/2\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/kaggle/input/phi-3/pytorch/phi-3.5-mini-instruct/2\",\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46092c4",
   "metadata": {
    "papermill": {
     "duration": 0.026359,
     "end_time": "2025-02-14T08:26:56.572471",
     "exception": false,
     "start_time": "2025-02-14T08:26:56.546112",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style=\"background-color:#fff1d7; padding:15px; \"> <b> Note:</b> You'll receive a warning that the flash-attention package is not found. That's because flash attention requires certain types of GPU hardware to run. Since the model of this lab is not using any GPU, you can ignore this warning.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fdb4f3",
   "metadata": {
    "papermill": {
     "duration": 0.027014,
     "end_time": "2025-02-14T08:26:56.626117",
     "exception": false,
     "start_time": "2025-02-14T08:26:56.599103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now you can wrap the model and the tokenizer in a [pipeline](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.pipeline) object that has \"text-generation\" as task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8765a5e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T08:26:56.681571Z",
     "iopub.status.busy": "2025-02-14T08:26:56.681129Z",
     "iopub.status.idle": "2025-02-14T08:26:56.688126Z",
     "shell.execute_reply": "2025-02-14T08:26:56.686809Z"
    },
    "height": 181,
    "papermill": {
     "duration": 0.036465,
     "end_time": "2025-02-14T08:26:56.690024",
     "exception": false,
     "start_time": "2025-02-14T08:26:56.653559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False, # False means to not include the prompt text in the returned text\n",
    "    max_new_tokens=300, \n",
    "    do_sample=False, # no randomness in the generated text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d79bc8",
   "metadata": {
    "id": "REqcz-ID_XgV",
    "papermill": {
     "duration": 0.02627,
     "end_time": "2025-02-14T08:26:56.748986",
     "exception": false,
     "start_time": "2025-02-14T08:26:56.722716",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generating a Text Response to a Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe18df1",
   "metadata": {
    "papermill": {
     "duration": 0.076639,
     "end_time": "2025-02-14T08:26:56.852654",
     "exception": false,
     "start_time": "2025-02-14T08:26:56.776015",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You'll now use the pipeline object (labeled as generator) to generate a response consisting of 50 tokens to the given prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e49812c",
   "metadata": {
    "papermill": {
     "duration": 0.027075,
     "end_time": "2025-02-14T08:26:56.906620",
     "exception": false,
     "start_time": "2025-02-14T08:26:56.879545",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ⏳ <b>Note: </b> The model might take around 2 minutes to generate the output.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68285ddf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T08:26:56.960859Z",
     "iopub.status.busy": "2025-02-14T08:26:56.960527Z",
     "iopub.status.idle": "2025-02-14T08:45:16.865073Z",
     "shell.execute_reply": "2025-02-14T08:45:16.863419Z"
    },
    "executionInfo": {
     "elapsed": 4955,
     "status": "ok",
     "timestamp": 1718959896168,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "height": 113,
    "id": "17h6TPHluJ-i",
    "outputId": "18727eeb-ccd6-40f8-aab1-25c8d9a03cbe",
    "papermill": {
     "duration": 1099.968944,
     "end_time": "2025-02-14T08:45:16.902565",
     "exception": false,
     "start_time": "2025-02-14T08:26:56.933621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Subject: Sincere Apologies for the Gardening Mishap\n",
      "\n",
      "Dear Sarah,\n",
      "\n",
      "I hope this message finds you well. I am writing to express my deepest apologies for the unfortunate incident that occurred in your garden yesterday.\n",
      "\n",
      "As you know, I had been looking forward to helping you with your gardening project. Unfortunately, during our time together, a series of unforeseen events led to an accident that resulted in some damage to your beautiful garden.\n",
      "\n",
      "The incident began when I was attempting to help you transplant a young sapling into its new location. In my eagerness to ensure the plant's successful relocation, I accidentally knocked over a nearby pot of delicate flowers. The pot shattered on impact, scattering soil and broken pottery across your meticulously maintained garden beds.\n",
      "\n",
      "I understand that this incident has caused you distress and inconvenience. Please know that I am truly sorry for any upset this may have caused you. I take full responsibility for my actions and the subsequent damage to your garden.\n",
      "\n",
      "To make amends, I would like to offer my assistance in repairing the damage. I am more than willing to help you clean up the broken pottery, replant the displaced flowers, and restore your garden to its former beauty. I will also take extra care to ensure that such an accident does not happen again in\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened. \"\n",
    "\n",
    "output = generator(prompt)\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f0db6e",
   "metadata": {
    "papermill": {
     "duration": 0.028549,
     "end_time": "2025-02-14T08:45:16.960402",
     "exception": false,
     "start_time": "2025-02-14T08:45:16.931853",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Exploring the Model's Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62729260",
   "metadata": {
    "papermill": {
     "duration": 0.028126,
     "end_time": "2025-02-14T08:45:17.017273",
     "exception": false,
     "start_time": "2025-02-14T08:45:16.989147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You can print the model to take a look at its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2493e831",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T08:45:17.076561Z",
     "iopub.status.busy": "2025-02-14T08:45:17.076092Z",
     "iopub.status.idle": "2025-02-14T08:45:17.086618Z",
     "shell.execute_reply": "2025-02-14T08:45:17.085563Z"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1718959898745,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "height": 30,
    "id": "eoFkdTd6_g5o",
    "outputId": "bdcfde9f-28b7-4f43-ec0c-32c16677a776",
    "papermill": {
     "duration": 0.042643,
     "end_time": "2025-02-14T08:45:17.088457",
     "exception": false,
     "start_time": "2025-02-14T08:45:17.045814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm()\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcb4c25",
   "metadata": {
    "papermill": {
     "duration": 0.029084,
     "end_time": "2025-02-14T08:45:17.146727",
     "exception": false,
     "start_time": "2025-02-14T08:45:17.117643",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The vocabulary size is 32064 tokens, and the size of the vector embedding for each token is 3072."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "487bd8b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T08:45:17.209632Z",
     "iopub.status.busy": "2025-02-14T08:45:17.209249Z",
     "iopub.status.idle": "2025-02-14T08:45:17.215071Z",
     "shell.execute_reply": "2025-02-14T08:45:17.214108Z"
    },
    "height": 30,
    "papermill": {
     "duration": 0.037038,
     "end_time": "2025-02-14T08:45:17.216543",
     "exception": false,
     "start_time": "2025-02-14T08:45:17.179505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32064, 3072, padding_idx=32000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.embed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0739d8e8",
   "metadata": {
    "papermill": {
     "duration": 0.028313,
     "end_time": "2025-02-14T08:45:17.273704",
     "exception": false,
     "start_time": "2025-02-14T08:45:17.245391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You can just focus on printing the stack of transformer blocks without the LM head component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9e33fc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T08:45:17.353770Z",
     "iopub.status.busy": "2025-02-14T08:45:17.353364Z",
     "iopub.status.idle": "2025-02-14T08:45:17.362705Z",
     "shell.execute_reply": "2025-02-14T08:45:17.361514Z"
    },
    "height": 30,
    "papermill": {
     "duration": 0.059791,
     "end_time": "2025-02-14T08:45:17.364676",
     "exception": false,
     "start_time": "2025-02-14T08:45:17.304885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3Model(\n",
       "  (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "  (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x Phi3DecoderLayer(\n",
       "      (self_attn): Phi3Attention(\n",
       "        (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "        (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): Phi3MLP(\n",
       "        (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "        (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        (activation_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): Phi3RMSNorm()\n",
       "      (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (post_attention_layernorm): Phi3RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): Phi3RMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b418cf",
   "metadata": {
    "papermill": {
     "duration": 0.028676,
     "end_time": "2025-02-14T08:45:17.431981",
     "exception": false,
     "start_time": "2025-02-14T08:45:17.403305",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are 32 transformer blocks or layers. You can access any particular block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adc37818",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T08:45:17.490297Z",
     "iopub.status.busy": "2025-02-14T08:45:17.489939Z",
     "iopub.status.idle": "2025-02-14T08:45:17.496026Z",
     "shell.execute_reply": "2025-02-14T08:45:17.495181Z"
    },
    "height": 30,
    "papermill": {
     "duration": 0.037264,
     "end_time": "2025-02-14T08:45:17.497654",
     "exception": false,
     "start_time": "2025-02-14T08:45:17.460390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3DecoderLayer(\n",
       "  (self_attn): Phi3Attention(\n",
       "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
       "  )\n",
       "  (mlp): Phi3MLP(\n",
       "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "    (activation_fn): SiLU()\n",
       "  )\n",
       "  (input_layernorm): Phi3RMSNorm()\n",
       "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (post_attention_layernorm): Phi3RMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4ffb9f",
   "metadata": {
    "id": "RTrwzB67BYVY",
    "papermill": {
     "duration": 0.029067,
     "end_time": "2025-02-14T08:45:17.555572",
     "exception": false,
     "start_time": "2025-02-14T08:45:17.526505",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generating a Single Token to a Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc6be21",
   "metadata": {
    "papermill": {
     "duration": 0.028396,
     "end_time": "2025-02-14T08:45:17.612602",
     "exception": false,
     "start_time": "2025-02-14T08:45:17.584206",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You earlier used the Pipeline object to generate a text response to a prompt. The pipeline provides an abstraction to the underlying process of text generation. Each token in the text is actually generated one by one. \n",
    "\n",
    "Let's now give the model a prompt and check the first token it will generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a90c247",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T08:45:17.669914Z",
     "iopub.status.busy": "2025-02-14T08:45:17.669579Z",
     "iopub.status.idle": "2025-02-14T08:45:17.673570Z",
     "shell.execute_reply": "2025-02-14T08:45:17.672632Z"
    },
    "height": 30,
    "id": "sEcxYgJxBYbJ",
    "papermill": {
     "duration": 0.034595,
     "end_time": "2025-02-14T08:45:17.675183",
     "exception": false,
     "start_time": "2025-02-14T08:45:17.640588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"The capital of Nigeria is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690c9aaa",
   "metadata": {
    "papermill": {
     "duration": 0.028299,
     "end_time": "2025-02-14T08:45:17.732658",
     "exception": false,
     "start_time": "2025-02-14T08:45:17.704359",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You'll need first to tokenize the prompt and get the ids of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e77392d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T08:45:17.790950Z",
     "iopub.status.busy": "2025-02-14T08:45:17.790590Z",
     "iopub.status.idle": "2025-02-14T08:45:17.830468Z",
     "shell.execute_reply": "2025-02-14T08:45:17.829471Z"
    },
    "height": 79,
    "papermill": {
     "duration": 0.070854,
     "end_time": "2025-02-14T08:45:17.832078",
     "exception": false,
     "start_time": "2025-02-14T08:45:17.761224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  450,  7483,   310, 20537,   423,   338]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the input prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1142797e",
   "metadata": {
    "papermill": {
     "duration": 0.02874,
     "end_time": "2025-02-14T08:45:17.890067",
     "exception": false,
     "start_time": "2025-02-14T08:45:17.861327",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's now pass the token ids to the transformer block (before the LM head)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60f0daed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T08:45:17.949759Z",
     "iopub.status.busy": "2025-02-14T08:45:17.949365Z",
     "iopub.status.idle": "2025-02-14T08:45:36.431760Z",
     "shell.execute_reply": "2025-02-14T08:45:36.430794Z"
    },
    "height": 62,
    "papermill": {
     "duration": 18.514669,
     "end_time": "2025-02-14T08:45:36.433727",
     "exception": false,
     "start_time": "2025-02-14T08:45:17.919058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the output of the model before the lm_head\n",
    "model_output = model.model(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bef77b9",
   "metadata": {
    "papermill": {
     "duration": 0.028736,
     "end_time": "2025-02-14T08:45:36.491896",
     "exception": false,
     "start_time": "2025-02-14T08:45:36.463160",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The transformer block outputs for each token a vector of size 3072 (embedding size). Let's check the shape of this output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ea018ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T08:45:36.551475Z",
     "iopub.status.busy": "2025-02-14T08:45:36.551025Z",
     "iopub.status.idle": "2025-02-14T08:45:36.557027Z",
     "shell.execute_reply": "2025-02-14T08:45:36.556165Z"
    },
    "height": 62,
    "papermill": {
     "duration": 0.037635,
     "end_time": "2025-02-14T08:45:36.558590",
     "exception": false,
     "start_time": "2025-02-14T08:45:36.520955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 3072])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the shape the output the model before the lm_head\n",
    "model_output[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56ebef5",
   "metadata": {
    "papermill": {
     "duration": 0.028354,
     "end_time": "2025-02-14T08:45:36.616647",
     "exception": false,
     "start_time": "2025-02-14T08:45:36.588293",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The first number represents the batch size, which is 1 in this case since we have one prompt. The second number 5 represents the number of tokens. And finally 3072 represents the embedding size (the size of the vector that corresponds to each token). \n",
    "\n",
    "Let's now get the output of the LM head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2e07147",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T08:45:36.674984Z",
     "iopub.status.busy": "2025-02-14T08:45:36.674633Z",
     "iopub.status.idle": "2025-02-14T08:45:37.177904Z",
     "shell.execute_reply": "2025-02-14T08:45:37.176979Z"
    },
    "height": 62,
    "papermill": {
     "duration": 0.534834,
     "end_time": "2025-02-14T08:45:37.179684",
     "exception": false,
     "start_time": "2025-02-14T08:45:36.644850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the output of the lm_head\n",
    "lm_head_output = model.lm_head(model_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e6e20e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T08:45:37.238917Z",
     "iopub.status.busy": "2025-02-14T08:45:37.238533Z",
     "iopub.status.idle": "2025-02-14T08:45:37.244246Z",
     "shell.execute_reply": "2025-02-14T08:45:37.243195Z"
    },
    "executionInfo": {
     "elapsed": 1079,
     "status": "ok",
     "timestamp": 1718960424560,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "height": 30,
    "id": "nC1PdOnTBnxZ",
    "outputId": "1fd5f482-7046-4536-b745-4e681d6ecdaf",
    "papermill": {
     "duration": 0.036966,
     "end_time": "2025-02-14T08:45:37.245728",
     "exception": false,
     "start_time": "2025-02-14T08:45:37.208762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 32064])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7078e3f7",
   "metadata": {
    "papermill": {
     "duration": 0.032707,
     "end_time": "2025-02-14T08:45:37.307216",
     "exception": false,
     "start_time": "2025-02-14T08:45:37.274509",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The LM head outputs for each token in the input prompt, a vector of size 32064 (vocabulary size). So there are 5 vectors, each of size 32064. Each vector can be mapped to a probability distribution, that shows the probability for each token in the vocabulary to come after the given token in the input prompt.\n",
    "\n",
    "Since we're interested in generating the output token that comes after the last token in the input prompt (\"is\"), we'll focus on the last vector. So in the next cell, `lm_head_output[0,-1]` is a vector of size 32064 from which you can generate the token that comes after (\"is\"). You can do that by finding the id of the token that corresponds to the highest value in the vector `lm_head_output[0,-1]` (using `argmax(-1)`, -1 means across the last axis here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d9efd63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T08:45:37.398126Z",
     "iopub.status.busy": "2025-02-14T08:45:37.397727Z",
     "iopub.status.idle": "2025-02-14T08:45:37.409464Z",
     "shell.execute_reply": "2025-02-14T08:45:37.408455Z"
    },
    "executionInfo": {
     "elapsed": 421,
     "status": "ok",
     "timestamp": 1718960391623,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "height": 47,
    "id": "68YUSS4GBf9Q",
    "outputId": "2dc25e8d-03b6-4bca-b46c-fec3e3a4a492",
    "papermill": {
     "duration": 0.060106,
     "end_time": "2025-02-14T08:45:37.411095",
     "exception": false,
     "start_time": "2025-02-14T08:45:37.350989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1976)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_id = lm_head_output[0,-1].argmax(-1)\n",
    "token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b9e1fe",
   "metadata": {
    "papermill": {
     "duration": 0.029021,
     "end_time": "2025-02-14T08:45:37.469710",
     "exception": false,
     "start_time": "2025-02-14T08:45:37.440689",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Finally, let's decode the returned token id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64d96449",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T08:45:37.529953Z",
     "iopub.status.busy": "2025-02-14T08:45:37.529526Z",
     "iopub.status.idle": "2025-02-14T08:45:37.535748Z",
     "shell.execute_reply": "2025-02-14T08:45:37.534692Z"
    },
    "height": 30,
    "papermill": {
     "duration": 0.037915,
     "end_time": "2025-02-14T08:45:37.537280",
     "exception": false,
     "start_time": "2025-02-14T08:45:37.499365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ab'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbfd5c4",
   "metadata": {
    "papermill": {
     "duration": 0.029728,
     "end_time": "2025-02-14T08:45:37.595951",
     "exception": false,
     "start_time": "2025-02-14T08:45:37.566223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1eef5a09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T08:45:37.655505Z",
     "iopub.status.busy": "2025-02-14T08:45:37.655101Z",
     "iopub.status.idle": "2025-02-14T09:20:53.604393Z",
     "shell.execute_reply": "2025-02-14T09:20:53.602768Z"
    },
    "papermill": {
     "duration": 2116.016082,
     "end_time": "2025-02-14T09:20:53.641166",
     "exception": false,
     "start_time": "2025-02-14T08:45:37.625084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# Answer\n",
      "**Situation:**\n",
      "At an e-commerce company with a team of 20, I was tasked with enhancing our sales forecasting capabilities. The company aimed to better understand customer behavior to drive sales growth.\n",
      "\n",
      "**Task:**\n",
      "I collaborated with the sales and marketing teams to implement a machine learning model that could accurately predict customer behaviors and sales trends.\n",
      "\n",
      "**Action:**\n",
      "To build the predictive model, I began by sourcing and cleaning the data from our stakeholders. I meticulously checked for outliers and handled missing data using median imputation. I then split the dataset into training and testing sets and normalized the data to prepare it for various algorithms.\n",
      "\n",
      "I experimented with different machine learning algorithms, including Multilayer Perceptron (MLP), logistic regression, and random forest, to find the most effective model for our classification problem. After training the models, I evaluated their performance using precision, recall, F1-score, and ROC-AUC metrics.\n",
      "\n",
      "The model I selected achieved an accuracy of 76%, which translated into a significant 15% increase in sales for the company.\n",
      "\n",
      "**Result:**\n",
      "The implementation of the predictive model had a direct and positive impact on the company's business operations. The sales team was able to make more informed decisions based\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Help rewrite this using a STAR interview method: I worked with an e-commerce company with a size of 20 people. The company need to predict customer behaviour and sales. I stepped into the project collaborating with the sales and marketing team to apply a machine-learning model to predict customer behaviours and sales. I sourced my data from existing stakeholders involved, cleaned the data, checked for outliers, split the dataset into train and test sets, normalised the provided dataset employed different algorithms i.e. MLP, logistic regression and random forest, the reason for this is the problem I'm solving is a classification problem and I made predictions on the test set. I employed evaluation metrics to measure the model's performance using precision, recall, F1-score, and ROC-AUC. The model achieved an accuracy of 76%, resulting in a 15% increase in sales. One of the challenges encountered was outliers and missing data points, I applied a median imputation to handle the missing data and remove features with the high outliers. The company business was impacted as they made required adjustments based on the sales prediction, I created an interactive dashboard using PowerBI to show the predicted sales revenue over time, with filters for different product categories and regions. The stakeholders, including the sales and marketing teams, could use the dashboard to explore the data, identify trends, and make data-driven decisions. I received positive feedback from the stakeholders on the effectiveness of the visualizations in communicating the insights and results.\"\n",
    "\n",
    "output = generator(prompt)\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e2c2332",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T09:20:53.800837Z",
     "iopub.status.busy": "2025-02-14T09:20:53.800450Z",
     "iopub.status.idle": "2025-02-14T09:25:58.790632Z",
     "shell.execute_reply": "2025-02-14T09:25:58.789453Z"
    },
    "papermill": {
     "duration": 305.150064,
     "end_time": "2025-02-14T09:25:58.821186",
     "exception": false,
     "start_time": "2025-02-14T09:20:53.671122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A) Lagos\n",
      "B) Abuja\n",
      "C) Kano\n",
      "D) Port Harcourt\n",
      "\n",
      "# Answer\n",
      "B) Abuja\n",
      "\n",
      "Abuja was designated as the capital of Nigeria in 1991, replacing Lagos. It is located in the center of the country and was chosen for its central location, which is more accessible to all parts of the nation.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of Nigeria?\"\n",
    "output = generator(prompt)\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fa6d76",
   "metadata": {
    "papermill": {
     "duration": 0.027585,
     "end_time": "2025-02-14T09:25:58.877124",
     "exception": false,
     "start_time": "2025-02-14T09:25:58.849539",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style=\"background-color:#f2f2ff; padding:15px; border-width:3px; border-color:#e2e2ff; border-style:solid; border-radius:6px\"> ⬇\n",
    "&nbsp; <b>Download Notebooks:</b> If you'd like to donwload the notebook: 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>. For more help, please see the <em>\"Appendix – Tips, Help, and Download\"</em> Lesson.</p>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 163622,
     "modelInstanceId": 141018,
     "sourceId": 165740,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3582.140293,
   "end_time": "2025-02-14T09:26:01.540766",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-14T08:26:19.400473",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "4379dc751bdb45e9a4995b0aa43b35b5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6dd9efba86cc491e80f24f7ab6a41bae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f32a2a7da92a4d6abf8e128a5ec9b786",
        "IPY_MODEL_d2f93ba26f884dab9b6b557b9b468b71",
        "IPY_MODEL_f2c72ae459c249f99213c7c1334b6d79"
       ],
       "layout": "IPY_MODEL_75c4acca166e4b52a5a36b1d3faa3636",
       "tabbable": null,
       "tooltip": null
      }
     },
     "6f600afb6e6843db84c762f746badeee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "75c4acca166e4b52a5a36b1d3faa3636": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b5109a266d8e42e3a799d980e29b4be5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b67a83ccc93647e6989b2054b130a455": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c038cd2be3814ac7900398ad003be7b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d2f93ba26f884dab9b6b557b9b468b71": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6f600afb6e6843db84c762f746badeee",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b67a83ccc93647e6989b2054b130a455",
       "tabbable": null,
       "tooltip": null,
       "value": 2.0
      }
     },
     "debcdaa14d1b4d55a108d80d70dc3b4e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f2c72ae459c249f99213c7c1334b6d79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_debcdaa14d1b4d55a108d80d70dc3b4e",
       "placeholder": "​",
       "style": "IPY_MODEL_b5109a266d8e42e3a799d980e29b4be5",
       "tabbable": null,
       "tooltip": null,
       "value": " 2/2 [00:01&lt;00:00,  1.51it/s]"
      }
     },
     "f32a2a7da92a4d6abf8e128a5ec9b786": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4379dc751bdb45e9a4995b0aa43b35b5",
       "placeholder": "​",
       "style": "IPY_MODEL_c038cd2be3814ac7900398ad003be7b4",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
